# -*- coding: utf-8 -*-
"""Mohammad Radya Fariez_VIX_ID/X Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QqACF7iUmgsEiS5pKFyw_B9NAjlT0__J
"""

#Final Task

# Commented out IPython magic to ensure Python compatibility.
#import lib yang digunakan
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

#load dataset
from google.colab import files
uploaded = files.upload()

dataset = pd.read_csv('loan_data_2007_2014.csv')

#5 First Row
dataset.head()

dataset.shape

dataset.dtypes

dataset.info()

drop_col = ['Unnamed: 0', 'annual_inc_joint', 'dti_joint', 'verification_status_joint',
       'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m',
       'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m',
       'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl',
       'inq_last_12m']

data = dataset.drop(columns=drop_col, axis=1)

# Check loan status
data.loan_status.value_counts()

#define values
ambiguous = ['Current', 'In Grace Period']
good_loan =  ['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid']

#drop baris yg berakhiran ambigu
data = data[data.loan_status.isin(ambiguous) == False]

#create kolom baru
data['loan_ending'] = np.where(data['loan_status'].isin(good_loan), 'good', 'risky')

import seaborn as sns

# check the balance
plt.title('good vs risky loans balance')
sns.barplot(x=data.loan_ending.value_counts().index,y=data.loan_ending.value_counts().values)

data.columns

leakage_col = ['issue_d', 'loan_status', 'pymnt_plan', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 
                   'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 
                   'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d']

data.drop(columns=leakage_col, axis=1, inplace=True)

# check duplicated data
data[['loan_amnt','funded_amnt','funded_amnt_inv','grade','sub_grade','desc','purpose','title']]

data[['loan_amnt','funded_amnt','funded_amnt_inv']].describe()

# based on the output, the data is so similar, and we can remove 2 of them + the other columns explained above.
drop_col = ['funded_amnt', 'funded_amnt_inv', 'id', 'member_id', 'url', 'desc']
dropped_data = data[drop_col]

data.drop(columns=drop_col, axis=1, inplace=True)

#explore data
data.info()

personal_record = ['mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog']

data[personal_record]

cols = ['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']

# pivot table aggregated by mean
print(pd.pivot_table(data, index = 'loan_ending', values = cols))

# pivot table aggregated by max value
print(pd.pivot_table(data, index = 'loan_ending', values = cols, aggfunc = np.max))

data[cols].describe()

plt.figure(figsize=(10,6))

# Using "> 0" because 75% of the data is 0... so the plot below just use < 25% of the data
sns.kdeplot(data = data[(data['tot_coll_amt'] < 100000) & (data['tot_coll_amt'] > 0)], x='tot_coll_amt', hue='loan_ending')

plt.figure(figsize=(10,6))
sns.kdeplot(data=data[data['tot_cur_bal'] < 800000], x='tot_cur_bal', hue='loan_ending')

plt.figure(figsize=(10,6))
sns.kdeplot(data=data[data['total_rev_hi_lim'] < 250000], x='total_rev_hi_lim', hue='loan_ending')

drop_col = ['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']

dropped_data = pd.concat([dropped_data, data[drop_col]], axis = 1)
data.drop(drop_col, inplace=True, axis = 1)

#Data with small unique value (less than 10 unique values)
data.nunique()[data.nunique() < 10].sort_values()

data.drop(['policy_code','application_type'], inplace=True, axis = 1)

#Create ratio
def risk_pct_chart(x):
    ratio = (data.groupby(x)['loan_ending'] # group by
         .value_counts(normalize=True) # calculate the ratio
         .mul(100) # multiply by 100 to be percent
         .rename('risky_pct') # rename column as percent
         .reset_index())

    sns.lineplot(data=ratio[ratio['loan_ending'] == 'risky'], x=x, y='risky_pct')
    plt.title(x)
    plt.show()

print(data.nunique()[data.nunique() < 10].sort_values().index)

small_unique = ['term', 'initial_list_status', 'major_derogatory',
       'verification_status', 'home_ownership', 'acc_now_delinq', 'grade',
       'collections_12_mths_ex_med']

for cols in small_unique:
    risk_pct_chart(cols)

#cleaning with numerical and categorical data
# numerical
num_data = data.select_dtypes(exclude= 'object')
num_data.columns

# categorical
cat_data = data.select_dtypes(include= 'object')
cat_data.columns

cols = ['emp_length', 'earliest_cr_line', 'last_credit_pull_d']

cat_data[cols].head()

# Check unique value to map
data['emp_length'].unique()

emp_map = {
    '< 1 year' : '0',
    '1 year' : '1',
    '2 years' : '2',
    '3 years' : '3',
    '4 years' : '4',
    '5 years' : '5',
    '6 years' : '6',
    '7 years' : '7',
    '8 years' : '8',
    '9 years' : '9',
    '10+ years' : '10'
}

data['emp_length'] = data['emp_length'].map(emp_map).fillna('0').astype(int)
data['emp_length'].unique()

# Pick just year from earliest credit line
data['earliest_cr_yr'] = pd.to_datetime(data['earliest_cr_line'], format = "%b-%y").dt.year

# calculate year since last inquiry
data['yr_since_last_inq'] = 2016 - pd.to_datetime(data['last_credit_pull_d'], format = "%b-%y").dt.year

data[['emp_length', 'earliest_cr_yr', 'yr_since_last_inq']].describe()

#the latest credit line should be around 2014-2015.
data = data[data['earliest_cr_yr'] < 2016]

to_chart = ['emp_length', 'earliest_cr_yr', 'yr_since_last_inq']

for cols in to_chart:
    risk_pct_chart(cols)

to_drop = ['earliest_cr_line', 'last_credit_pull_d']
dropped_data = pd.concat([dropped_data, data[to_drop]], axis = 1)

# numerical
num_data = data.drop(to_drop, axis=1).select_dtypes(exclude= 'object')
print('num data: ', num_data.columns)

# categorical
cat_data = data.drop(to_drop, axis=1).select_dtypes(include= 'object')
print('cat data: ', cat_data.columns)

# Distribution
for i in num_data.columns:
    plt.hist(num_data[i])
    plt.title(i)
    plt.show()

# Correlation
plt.figure(figsize=(14,6))
sns.heatmap(data=num_data.corr(), annot=True)

# Pivot table
pd.pivot_table(data, index = 'loan_ending', values = num_data.columns)

cat_data.nunique()

to_drop = ['zip_code', 'title', 'emp_title']
dropped_data = pd.concat([dropped_data, data[to_drop]], axis = 1) # It's my habit to collect dropped data

cat_data.drop(to_drop, axis=1, inplace=True)

cat_data.columns

to_chart = ['grade', 'sub_grade', 'home_ownership', 'verification_status', 'purpose', 'addr_state']

for cols in to_chart:
    plt.figure(figsize=(14,4))
    risk_pct_chart(cols)

dropped_data = pd.concat([dropped_data, data['sub_grade']], axis = 1)
cat_data.drop('sub_grade', axis = 1, inplace=True)
cat_data.nunique()

# Transforming 'term'
cat_data['term'] = cat_data['term'].str.replace(' months', '').astype(int)

cat_data['grade'].unique()

#transforming grade
grade_map = {
    'A' : 1,
    'B' : 2,
    'C' : 3,
    'D' : 4,
    'E' : 5,
    'F' : 6,
    'G' : 7,
}

cat_data['grade'] = cat_data['grade'].map(grade_map)

# one hot encode
to_dummies = ['home_ownership', 'verification_status', 'purpose', 'addr_state', 'initial_list_status']

dummies = pd.get_dummies(cat_data[to_dummies])
dummies.drop('initial_list_status_w', axis=1, inplace=True)

dummies.head()

# dropping columns that already one hot encoded
dropped_data = pd.concat([dropped_data, cat_data[to_dummies]], axis = 1)
cat_data.drop(to_dummies, axis=1, inplace=True)

# combining categorical data with one hot encoded data
cat_data_f = pd.concat([cat_data, dummies], axis = 1)

# combining numerical and categorical data
final_data = pd.concat([num_data, cat_data_f], axis = 1).dropna().reset_index().drop('index', axis = 1)
final_data.head()

# separate dependant (y) and independant (X) variable
X = final_data.drop('loan_ending', axis = 1)
y = final_data['loan_ending']

# splitting for training and model validation, it's important to avoid overfitting
from sklearn.model_selection import train_test_split

train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)

# kaggle exclusive
kaggle_train_y = np.where(train_y == 'good', 1, 0)
kaggle_val_y = np.where(val_y == 'good', 1, 0)

# Modelng Section
from sklearn.model_selection import cross_val_score
from sklearn import tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier

# Evaluation
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

pred_y = np.where(val_y == 'good',1,1)
print(classification_report(kaggle_val_y, pred_y))

# Decision tree
dt = tree.DecisionTreeClassifier(random_state = 14)
dt.fit(train_X, kaggle_train_y)
pred_y = dt.predict(val_X)
print(classification_report(kaggle_val_y, pred_y))

# KNeirestNeighbor
knn = KNeighborsClassifier()
knn.fit(train_X, kaggle_train_y)
pred_y = knn.predict(val_X)
print(classification_report(kaggle_val_y, pred_y))

# Random Forest
rf = RandomForestClassifier(random_state = 14)
rf.fit(train_X, kaggle_train_y)
pred_y = rf.predict(val_X)
print(classification_report(kaggle_val_y, pred_y))

# XGB
xgb = XGBClassifier(random_state = 14)
xgb.fit(train_X, kaggle_train_y)
pred_y = xgb.predict(val_X)
print(classification_report(kaggle_val_y, pred_y))

# ensemble soft voting classifier
voting_clf = VotingClassifier(estimators = [('knn',knn),('rf',rf),('xgb',xgb)], voting = 'soft')
voting_clf.fit(train_X, kaggle_train_y)
pred_y = voting_clf.predict(val_X)
print(classification_report(kaggle_val_y, pred_y))